---
title: "Exploratory Data Analysis of Processed HC Corpora"
author: "KevinHo"
date: "11 April 2017"
output: html_document
---

```{r setup, echo=FALSE, warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(stringr)
library(tm)
library(ngram)
library(knitr)
source("Text Mining.R")
source("helpers.R")
```

```{r static variables, echo = FALSE}
samplePercentage = 0.1
```

The first step is to make a corpus for our use. Rather than process the entire corpus we can randomly sample the data to get a fairly accurate representation. This requires a sample files to be generated

```{r sampling function, echo = FALSE}
create.sampled.file <- function(inputFile, outputFile, fraction = samplePercentage) {
    #This function randomly samples lines from an inputFile and then exports it into an outputFile
    
    conn <- file(inputFile, "r")
    fileContents <- readLines(conn, encoding="UTF-16LE", skipNul = TRUE)
    nlines <- length(fileContents)
    close(conn)
    
    conn <- file(outputFile, "w")
    selection <- rbinom(nlines, 1, fraction)
    for(i in  1: nlines) {
        if (selection[i]==1) {cat( stringi::stri_trans_general(fileContents[i], "latin-ascii"), file=conn, sep = "\n")}
    }
    close(conn)
    
    paste("Saved", sum(selection), "lines to file.", outputFile)
}
```

`r samplePercentage*100`% of the original corpus will be taken as a sample

```{r sampling, warning=FALSE, echo=FALSE}

create.sampled.file("../../Temp/Data-Science-Specialization-Capstone-Data/en_US.news.txt","../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/news/en_US.news_Sampled.txt", fraction = samplePercentage)

create.sampled.file("../../Temp/Data-Science-Specialization-Capstone-Data/en_US.blogs.txt","../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/blogs/en_US.blogs_Sampled.txt", fraction = samplePercentage)

create.sampled.file("../../Temp/Data-Science-Specialization-Capstone-Data/en_US.twitter.txt","../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/twitter/en_US.twitter_Sampled.txt", fraction = samplePercentage)

```

##Source Data Analysis

Lets analyse each of the corpora individually to see if there are specific characteristics of each one. The folowing process also processes the corpora. It will leave only latin characters. All words are defined as continuous string of chracters without whitespaces. All punctuation and stopwords have been removed.

Because of this processing the problems of foreign words, typos and predicting the next word past a stop word still remain.
```{r makeDoc, warning=FALSE}
docs.twitter <- make.TMMap("C:/Users/Kevin/Documents/GitHub/Temp/Data-Science-Specialization-Capstone-Data/Sampled/twitter")
docs.blogs <- make.TMMap("C:/Users/Kevin/Documents/GitHub/Temp/Data-Science-Specialization-Capstone-Data/Sampled/blogs")
docs.news <- make.TMMap("C:/Users/Kevin/Documents/GitHub/Temp/Data-Science-Specialization-Capstone-Data/Sampled/news")

```

First we create Document Term Matricies for easier analysis

```{r staging}
dtm.twitter <- DocumentTermMatrix(docs.twitter)   
dtm.news <- DocumentTermMatrix(docs.news)   
dtm.blogs <- DocumentTermMatrix(docs.blogs)   
```

Lets have a look at the most frequent 20 terms of each dataset
```{r analysis,message=FALSE}
dtm.twitter.freq <- tbl_df(data.frame(Words = dtm.twitter$dimnames$Terms, Frequency = colSums(as.matrix(dtm.twitter))))
dtm.news.freq <- tbl_df(data.frame(Words = dtm.news$dimnames$Terms, Frequency = colSums(as.matrix(dtm.news))))
dtm.blogs.freq <- tbl_df(data.frame(Words = dtm.blogs$dimnames$Terms, Frequency = colSums(as.matrix(dtm.blogs))))

dtm.twitter.freq.top <- dtm.twitter.freq %>%
                        top_n(20)

dtm.news.freq.top <- dtm.news.freq %>%
                top_n(20)

dtm.blogs.freq.top <- dtm.blogs.freq %>%
                top_n(20)

#Fun bit of code so that ggplot will display the words ordered by their value
dtm.twitter.freq.top$Words <- factor(dtm.twitter.freq.top$Words, levels = dtm.twitter.freq.top$Words)
dtm.twitter.freq.top$Words <- factor(dtm.twitter.freq.top$Words, levels = dtm.twitter.freq.top$Words[order(dtm.twitter.freq.top$Frequency)])

dtm.news.freq.top$Words <- factor(dtm.news.freq.top$Words, levels = dtm.news.freq.top$Words)
dtm.news.freq.top$Words <- factor(dtm.news.freq.top$Words, levels = dtm.news.freq.top$Words[order(dtm.news.freq.top$Frequency)])

dtm.blogs.freq.top$Words <- factor(dtm.blogs.freq.top$Words, levels = dtm.blogs.freq.top$Words)
dtm.blogs.freq.top$Words <- factor(dtm.blogs.freq.top$Words, levels = dtm.blogs.freq.top$Words[order(dtm.blogs.freq.top$Frequency)])

g1 <- ggplot(dtm.twitter.freq.top, aes(Words,Frequency)) + geom_bar(stat = "Identity") + coord_flip() + labs(title = "Twitter")
g2 <- ggplot(dtm.blogs.freq.top, aes(Words,Frequency)) + geom_bar(stat = "Identity") + coord_flip() + labs(title = "Blogs")
g3 <- ggplot(dtm.news.freq.top, aes(Words,Frequency)) + geom_bar(stat = "Identity") + coord_flip() + labs(title = "News")

multiplot(g1,g2,g3, cols=3, title = "Top 20 Words")
```

It can be seen from the sample that the twitter and blogs data will dominate the news data in any predictive counts.

```{r}
uniqueWords <- data.frame("Source" = c("Twitter","Blogs","News"),
                          "Number of Words"= c(sum(dtm.twitter.freq$Frequency),sum(dtm.blogs.freq$Frequency),sum(dtm.news.freq$Frequency)),
                          "Unique Words"= c(nrow(dtm.twitter.freq),nrow(dtm.blogs.freq),nrow(dtm.news.freq)))

g1 <- ggplot(uniqueWords,aes(Source,Number.of.Words)) + geom_bar(stat = "Identity")
g2 <- ggplot(uniqueWords,aes(Source,Unique.Words)) + geom_bar(stat = "Identity")

multiplot(g1,g2, cols = 2)

```

Once again the news data will be under represented in any predictive tool based purely on counts.

## Analysis of the Corpus in N-grams

The aim of this project is to make an application that can take a phrase and predict the next most likely word. For example google search suggestions has a completion function. So when I type in "I want a" google offers the following suggestions


![Google Search: "I want a"](Google Suggestions.png)

This is the functionality we'd like to replicate.

Lets start with making a combined data source


```{r, message=FALSE, echo = FALSE, include = FALSE}
file.copy("../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/news/en_US.news_Sampled.txt","../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/en_US.news_Sampled.txt", overwrite = TRUE)
file.copy("../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/blogs/en_US.blogs_Sampled.txt","../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/en_US.blogs_Sampled.txt", overwrite = TRUE)
file.copy("../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/twitter/en_US.twitter_Sampled.txt","../../Temp/Data-Science-Specialization-Capstone-Data/Sampled/en_US.twitter_Sampled.txt", overwrite = TRUE)
```
```{r}
docs <- make.TMMap("C:/Users/Kevin/Documents/GitHub/Temp/Data-Science-Specialization-Capstone-Data/Sampled/")
```


And lets convert these documents into monograms, bigrams and trigrams.


```{r}
dtm.mono <- DocumentTermMatrix(docs)
dtm.bigram <- DocumentTermMatrix(docs, control = list(tokenize = BigramTokenizer))
dtm.trigram <- DocumentTermMatrix(docs, control = list(tokenize = TrigramTokenizer))
```

Lets have a look at the most frequent 20 terms of each dataset
```{r ngram analysis, cache=TRUE}
dtm.mono.freq <- tbl_df(data.frame(Words = dtm.mono$dimnames$Terms, Frequency = colSums(as.matrix(dtm.mono)))) %>% 
    arrange(desc(Frequency)) %>% 
    mutate(cumsum = cumsum(Frequency)) %>%
    mutate(coverage = cumsum/sum(Frequency))

dtm.bigram.freq <- tbl_df(data.frame(Words = dtm.bigram$dimnames$Terms, Frequency = colSums(as.matrix(dtm.bigram)))) %>%                
    arrange(desc(Frequency)) %>% 
    mutate(cumsum = cumsum(Frequency)) %>%
    mutate(coverage = cumsum/sum(Frequency))

dtm.trigram.freq <- tbl_df(data.frame(Words = dtm.trigram$dimnames$Terms, Frequency = colSums(as.matrix(dtm.trigram)))) %>% 
    arrange(desc(Frequency)) %>% 
    mutate(cumsum = cumsum(Frequency)) %>%
    mutate(coverage = cumsum/sum(Frequency))

dtm.mono.freq.top <- dtm.mono.freq %>%
                        top_n(20,Frequency)

dtm.bigram.freq.top <- dtm.bigram.freq %>%
                top_n(20,Frequency)

dtm.trigram.freq.top <- dtm.trigram.freq %>%
                top_n(20,Frequency)

#Fun bit of code so that ggplot will display the words ordered by their value
dtm.mono.freq.top$Words <- factor(dtm.mono.freq.top$Words, levels = dtm.mono.freq.top$Words)
dtm.mono.freq.top$Words <- factor(dtm.mono.freq.top$Words, levels = dtm.mono.freq.top$Words[order(dtm.mono.freq.top$Frequency)])

dtm.bigram.freq.top$Words <- factor(dtm.bigram.freq.top$Words, levels = dtm.bigram.freq.top$Words)
dtm.bigram.freq.top$Words <- factor(dtm.bigram.freq.top$Words, levels = dtm.bigram.freq.top$Words[order(dtm.bigram.freq.top$Frequency)])

dtm.trigram.freq.top$Words <- factor(dtm.trigram.freq.top$Words, levels = dtm.trigram.freq.top$Words)
dtm.trigram.freq.top$Words <- factor(dtm.trigram.freq.top$Words, levels = dtm.trigram.freq.top$Words[order(dtm.trigram.freq.top$Frequency)])

g1 <- ggplot(dtm.mono.freq.top, aes(Words,Frequency)) + geom_bar(stat = "Identity") + coord_flip() + labs(title = "Top 20 Words mono")
g2 <- ggplot(dtm.bigram.freq.top, aes(Words,Frequency)) + geom_bar(stat = "Identity") + coord_flip() + labs(title = "Top 20 Words bigram")
g3 <- ggplot(dtm.trigram.freq.top, aes(Words,Frequency)) + geom_bar(stat = "Identity") + coord_flip() + labs(title = "Top 20 Words trigram")

g1
g2
g3

```

It can be seen from the sample that as the length of the terms grow. The non-unique terms reduce by roughly an order of magnitude. This means that the dictionary size will need to grow to maintain coverage. As shown below.

```{r}
uniqueWords <- data.frame("Ngrams" = c("1","2","3"),
                          "Number of Words"= c(sum(dtm.mono.freq$Frequency),sum(dtm.bigram.freq$Frequency),sum(dtm.trigram.freq$Frequency)),
                          "Unique Words"= c(nrow(dtm.mono.freq),nrow(dtm.bigram.freq),nrow(dtm.trigram.freq)))

g1 <- ggplot(uniqueWords,aes(Ngrams,Number.of.Words)) + geom_bar(stat = "Identity")
g2 <- ggplot(uniqueWords,aes(Ngrams,Unique.Words)) + geom_bar(stat = "Identity")

multiplot(g1,g2, cols = 2)

```

Conversely the number of unique terms grow as the Ngrams get larger.

##Coverage analysis of N-Grams
Given the number of unique terms the following table shows how many terms are required to be included in the dictionary to cover 50% and 90% of terms used.
```{r coverageanalysis}

kable(
    tbl_df(
        data.frame(Ngram = c(1,2,3), 
            "Unique Words" = c(nrow(dtm.mono.freq), 
                             nrow(dtm.bigram.freq),
                             nrow(dtm.trigram.freq)
                             ), 
            "Fifty Percent" = c(nrow(dtm.mono.freq) - sum(dtm.mono.freq$coverage>.5),
                             nrow(dtm.bigram.freq) - sum(dtm.bigram.freq$coverage>.5),
                             nrow(dtm.trigram.freq) - sum(dtm.trigram.freq$coverage>.5)
                             ),
            "Ninety Percent" = c(nrow(dtm.mono.freq) - sum(dtm.mono.freq$coverage>.9),
                            nrow(dtm.bigram.freq) - sum(dtm.bigram.freq$coverage>.9),
                            nrow(dtm.trigram.freq) - sum(dtm.trigram.freq$coverage>.9)
                            )
                    )
        )
    ,
    caption = "Required dictionary size to attain % coverage", 
    col.names = c("N-Gram","Unique Words", "50% Coverage","90% Coverage")
    )
```

As observed in the table. As N gets large, the dictionary must also increase in size to attain suitable coverage. This may have storage and computational issues if the dictionary is too large.

By converting words into root words (infinitives) Coverage could increase. There is additional complexity in interpreting tense. eg should a word end in "er", "ing", "s, "ed" etc. and choosing the correct one would depend on the context of the sentence


#Appendix
File Sampling Function

```{r ref.label="sampling function", eval=FALSE}
```

Data sampling and cleaning functions
```{r code=readLines(knitr::purl('~/GitHub/Data-Science-Specialization-Capstone/Exploratory Data Analysis - Week 2/Text Mining.R', documentation = 0)), eval = FALSE}

```

Other helper functions. Including multiplot
```{r code=readLines(knitr::purl('~/GitHub/Data-Science-Specialization-Capstone/Exploratory Data Analysis - Week 2/helpers.R', documentation = 0)), eval = FALSE}

```
